{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89497b4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "key_path = \"user_id.json\"\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "\n",
    "# Optional: confirm it's set\n",
    "print(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c3cd7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CPL_VSIL_USE_TEMP_FILE_FOR_RANDOM_WRITE\"] = \"YES\"\n",
    "# --- Optional: faster temp dir if you have SSD space ---\n",
    "os.environ[\"TMPDIR\"] = \"/mnt/e\"  # or /mnt/nvme/temp_gdal on Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83384853",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# 2Ô∏è‚É£ Initialize GCS client\n",
    "client = storage.Client.from_service_account_json(key_path)\n",
    "\n",
    "# 3Ô∏è‚É£ List buckets (to verify connection)\n",
    "for bucket in client.list_buckets():\n",
    "    print(\"‚úÖ Found bucket:\", bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e0a9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = \"remote_sensing_saas\"\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "blobs = list(bucket.list_blobs(prefix=\"\"))  # list all files\n",
    "for blob in blobs:  # show only first 10\n",
    "    if '.tif' in blob.name:\n",
    "        print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff68c57",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from osgeo import gdal\n",
    "\n",
    "# --- CONFIG ---\n",
    "src = \"/mnt/d/KORINDO_DRONE/BLOK 1/ORTHO_PT_STL_KALBAR_Blok 1_TIFF.tif\"      # Input GeoTIFF in GCS\n",
    "dst = \"/vsigs/remote_sensing_saas/01-korindo/00-drone/ORTHO_PT_STL_KALBAR_Blok 1_TIFF_COG.tif\" # Output COG in GCS\n",
    "\n",
    "# --- START TIMER ---\n",
    "start = time.time()\n",
    "\n",
    "# --- DEFINE PROGRESS CALLBACK ---\n",
    "def progress_callback(complete, message, unknown):\n",
    "    percent = int(complete * 100)\n",
    "    print(f\"\\rProgress: {percent}% | {message}\", end=\"\")\n",
    "\n",
    "# --- RUN TRANSLATE ---\n",
    "print(f\"üöÄ Starting GDAL COG translation...\\nInput: {src}\\nOutput: {dst}\\n\")\n",
    "\n",
    "translate_options = [\n",
    "    \"OVERVIEWS=IGNORE_EXISTING\",\n",
    "    \"BIGTIFF=YES\", \n",
    "    \"COMPRESS=ZSTD\",\n",
    "    \"LEVEL=22\",\n",
    "    \"PREDICTOR=2\",\n",
    "    \"INTERLEAVE=BAND\",\n",
    "    \"NUM_THREADS=10\"\n",
    "]\n",
    "\n",
    "result = gdal.Translate(\n",
    "    dst,\n",
    "    src,\n",
    "    format=\"COG\",\n",
    "    creationOptions=translate_options,\n",
    "    callback=progress_callback\n",
    ")\n",
    "\n",
    "# --- END TIMER ---\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "if result is not None:\n",
    "    print(f\"\\n‚úÖ Done! COG successfully created in {elapsed:.2f} seconds.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå GDAL translate failed after {elapsed:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7e4b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### FOR LOWER SPEC\n",
    "import os\n",
    "import time\n",
    "from osgeo import gdal\n",
    "\n",
    "# --- GDAL Cloud Storage Fix (Crucial!) ---\n",
    "os.environ[\"CPL_VSIL_USE_TEMP_FILE_FOR_RANDOM_WRITE\"] = \"YES\"\n",
    "\n",
    "# --- 1. SET TEMP DIRECTORY ---\n",
    "# CRITICAL: You must MANUALLY CREATE this folder first!\n",
    "# Go to C:\\ and create a new folder named \"temp\".\n",
    "os.environ[\"CPL_TMPDIR\"] = r\"c:\\temp\" \n",
    "\n",
    "# --- OPTIMIZE FOR LOW RAM (8GB) ---\n",
    "gdal.SetConfigOption('GDAL_CACHEMAX', '1024')\n",
    "\n",
    "# --- CONFIG ---\n",
    "src = r\"C:\\Users\\treeo-workspace\\Documents\\BLOK 4\\ORTHO_PT_STL_KALBAR_Blok 4_TIFF.tif\"\n",
    "dst = r\"c:\\temp\\ORTHO_PT_STL_KALBAR_Blok 4_TIFF_COG.tif\"\n",
    "\n",
    "# --- START TIMER ---\n",
    "start = time.time()\n",
    "\n",
    "# --- DEFINE PROGRESS CALLBACK ---\n",
    "def progress_callback(complete, message, unknown):\n",
    "    percent = int(complete * 100)\n",
    "    print(f\"\\rProgress (local creation): {percent}% | {message}      \", end=\"\", flush=True)\n",
    "\n",
    "# --- 2. RUN TRANSLATE ---\n",
    "print(f\"üöÄ Starting GDAL COG translation...\\nInput: {src}\\nOutput: {dst}\\n\")\n",
    "\n",
    "translate_options = [\n",
    "    \"OVERVIEWS=IGNORE_EXISTING\",\n",
    "    \"BIGTIFF=YES\", \n",
    "    \"COMPRESS=ZSTD\",\n",
    "    \"LEVEL=9\",\n",
    "    \"PREDICTOR=2\",\n",
    "    \"INTERLEAVE=BAND\",\n",
    "    \"NUM_THREADS=6\" \n",
    "]\n",
    "\n",
    "result = gdal.Translate(\n",
    "    dst,\n",
    "    src,\n",
    "    format=\"COG\",\n",
    "    creationOptions=translate_options,\n",
    "    callback=progress_callback\n",
    ")\n",
    "\n",
    "# Ensure we print a newline after the progress bar\n",
    "print() \n",
    "\n",
    "# --- 3. THIS IS THE CORRECTED PART ---\n",
    "if result is not None:\n",
    "    print(\"‚úÖ Local COG creation complete.\")\n",
    "    print(\"‚è≥ Forcing upload to Google Cloud Storage... (This may take a while)\")\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Force GDAL to write all data and close the file handle.\n",
    "    # This blocks the script until the upload is 100% finished.\n",
    "    result.FlushCache()  \n",
    "    result = None        # This line triggers the actual upload\n",
    "    # --- END FIX ---\n",
    "    \n",
    "    # NOW we can safely stop the timer\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"‚úÖ Upload complete! Total time: {elapsed:.2f} seconds.\")\n",
    "    print(\"\\n--- Running verification... ---\")\n",
    "    \n",
    "    # --- 4. IMMEDIATE VERIFICATION ---\n",
    "    gdal.PushErrorHandler('CPLQuietErrorHandler') \n",
    "    ds_verify = gdal.Open(dst)\n",
    "    gdal.PopErrorHandler() \n",
    "\n",
    "    if ds_verify is not None:\n",
    "        print(\"‚úÖ‚úÖ‚úÖ SUCCESS! File is confirmed in the bucket.\")\n",
    "        ds_verify = None\n",
    "    else:\n",
    "        print(\"‚ùå‚ùå‚ùå FAILURE! Verification step failed. File not found in bucket.\")\n",
    "\n",
    "else:\n",
    "    # This means gdal.Translate failed locally\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"‚ùå GDAL translate failed during *local* creation after {elapsed:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a24b8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### UPLOADING TO GCS\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from tqdm.notebook import tqdm  # <-- Import tqdm for Jupyter\n",
    "\n",
    "def upload_file_to_gcs_with_progress(bucket_name, source_file_path, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Uploads a local file to GCS with a progress bar in Jupyter.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of your GCS bucket.\n",
    "        source_file_path (str): The local path to the file you want to upload.\n",
    "        destination_blob_name (str): The name you want the file to have in GCS.\n",
    "    \"\"\"\n",
    "    # 1. Instantiate the GCS client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # 2. Get the target bucket\n",
    "    try:\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting bucket '{bucket_name}': {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Create a new blob (file) in the bucket\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # 4. Get the total file size for the progress bar\n",
    "    try:\n",
    "        file_size = os.path.getsize(source_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {source_file_path} was not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file size: {e}\")\n",
    "        return\n",
    "\n",
    "    # 5. Upload the file using upload_from_file and the tqdm wrapper\n",
    "    try:\n",
    "        # Open the file in binary-read mode\n",
    "        with open(source_file_path, \"rb\") as f:\n",
    "            \n",
    "            # Create the tqdm wrapper\n",
    "            # - 'f' is the file object\n",
    "            # - \"read\" is the method to wrap\n",
    "            # - total is the total file size\n",
    "            # - unit=\"B\" and unit_scale=True create human-readable sizes (e.g., MB)\n",
    "            with tqdm.wrapattr(f, \"read\", total=file_size, desc=\"Uploading\", unit=\"B\", unit_scale=True, unit_divisor=1024) as file_with_progress:\n",
    "                \n",
    "                # Use upload_from_file, passing the wrapped object and the total size\n",
    "                blob.upload_from_file(file_with_progress, size=file_size)\n",
    "\n",
    "        print(f\"\\n‚úÖ Upload complete: {source_file_path} to {bucket_name}/{destination_blob_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during upload: {e}\")\n",
    "\n",
    "# --- How to use the function ---\n",
    "# (Assuming 'bucket_name' and 'dst' are defined in your Jupyter notebook)\n",
    "\n",
    "MY_BUCKET_NAME = bucket_name  # <-- Make sure 'bucket_name' is defined in a cell above\n",
    "dst = dst                     # <-- Make sure 'dst' (your file path) is defined in a cell above\n",
    "\n",
    "GCS_DESTINATION_NAME = f\"01-korindo/00-drone/{os.path.basename(dst)}\"\n",
    "\n",
    "###Call the new, correct function\n",
    "upload_file_to_gcs_with_progress(MY_BUCKET_NAME, dst, GCS_DESTINATION_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
