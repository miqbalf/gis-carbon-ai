{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf7ee1-8c1d-4c23-acff-0a62e693c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bb52a-b601-49d0-9fc8-b3defd3ebd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "\n",
    "# arcpy.SignInToPortal()\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.workspace = '00_input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dad72-2fce-474d-81e8-c274180a9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd3a72-f002-4f59-bfad-60f3dc0f3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944821f6-7872-4059-a402-4bf6e8bcf697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getenv(\"GOOGLE_CLOUD_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6592390-c634-4279-ad21-973e0aa05cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "gcs = gcsfs.GCSFileSystem(project=os.getenv(\"GOOGLE_CLOUD_PROJECT\"), token='../../backend/user_id.json')\n",
    "fs = gcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b0dd8-6d1a-4902-8932-cdd900e7e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DS TRAIN CHECK RESAMPLING\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "\n",
    "path_training = '00_input/training_shp/'\n",
    "\n",
    "layers = [shp for shp in os.listdir(path_training) if shp.endswith('.shp') and shp.startswith('sample')]\n",
    "gcs_path = 'gs://remote_sensing_saas/01-korindo/sample_tsfresh/20251112_training_gdf_col_filtered.parquet'\n",
    "\n",
    "use_parquet_training = True\n",
    "\n",
    "if use_parquet_training != True:\n",
    "    gdf_list = []\n",
    "\n",
    "    for lyr in layers:\n",
    "        print(lyr)\n",
    "    gdf = gpd.read_file(os.path.join(path_training,lyr))\n",
    "    gdf['layer'] = lyr.replace('.shp', '')\n",
    "    print(gdf.crs)\n",
    "    gdf_utm = gdf.to_crs(crs_ds)\n",
    "    gdf_list.append(gdf_utm)\n",
    "\n",
    "    training_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True))\n",
    "    # training_gdf = gpd.read_file('00_input/training_shp/sample_1.shp')\n",
    "    # training_gdf.head()\n",
    "    # training_gdf.head()\n",
    "\n",
    "    # training_gdf.crs\n",
    "\n",
    "    # training_gdf.geometry.head()\n",
    "    # training_gdf.columns\n",
    "\n",
    "    list_columns_time = [i for i in training_gdf.columns if i.startswith('t_') and not i.endswith('D')]\n",
    "    list_columns_time = list(sorted(list_columns_time))\n",
    "    # list_columns_time\n",
    "\n",
    "    columns_filter = ['layer'] + list_columns_time + ['geometry']\n",
    "\n",
    "    training_gdf_col_filtered = training_gdf[columns_filter]\n",
    "    # training_gdf_col_filtered.head()\n",
    "    # training_gdf_col_filtered.plot()\n",
    "\n",
    "    print(training_gdf_col_filtered.shape)\n",
    "\n",
    "    # Save as GeoParquet (BEST option)\n",
    "    gcs_path = gcs_path    \n",
    "    training_gdf_col_filtered.to_parquet(gcs_path, filesystem=fs, compression='snappy')\n",
    "\n",
    "else:\n",
    "    training_gdf_col_filtered = gpd.read_parquet(gcs_path, filesystem=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55fa44-3e83-4d70-b5ae-ca896283e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_gdf_col_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb57575-b6eb-4559-8ded-47e1fda65980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "use_existing_df_long = False # save some cost here to download from gcs\n",
    "#gcs_path_df_long = 'gs://remote_sensing_saas/01-korindo/sample_tsfresh/20251112_df_long.parquet'\n",
    "\n",
    "if use_existing_df_long != True:\n",
    "    # training_gdf_col_filtered\n",
    "\n",
    "    t_cols = [col for col in training_gdf_col_filtered.columns if col.startswith('t_')]\n",
    "\n",
    "    # Melt the dataframe with geometry as id_var\n",
    "    df_long = pd.melt(\n",
    "        training_gdf_col_filtered, \n",
    "        id_vars=['layer','geometry'],\n",
    "        value_vars=t_cols,\n",
    "        var_name='time_period',\n",
    "        value_name='value'\n",
    "    )\n",
    "\n",
    "    # Set geometry as index\n",
    "    # df_long = df_long.set_index('geometry')\n",
    "\n",
    "    print(f\"Original shape: {training_gdf_col_filtered.shape}\")\n",
    "    print(f\"Long format shape: {df_long.shape}\")\n",
    "    print(f\"Time columns found: {len(t_cols)}\")\n",
    "    # print(f\"\\nFirst few rows:\")\n",
    "    # df_long.head()\n",
    "    # df_long.layer.unique()\n",
    "\n",
    "    # df_long.reset_index(inplace=True)\n",
    "    # df_long.head()\n",
    "\n",
    "    ## reformating the column (date)\n",
    "    df_long = df_long.rename(columns={'value': 'type'})\n",
    "    df_long['date'] = df_long['time_period'].str[2:].astype(int)\n",
    "    # df_long.head()\n",
    "\n",
    "    # type(df_long)\n",
    "\n",
    "    training_gdf = df_long.copy()\n",
    "    # Remove multipolygons\n",
    "    # training_gdf = training_gdf.explode()\n",
    "    print(training_gdf.crs)\n",
    "\n",
    "    ##### conversion\n",
    "    # Drop rows where 'type' is NA\n",
    "    training_gdf = training_gdf.copy()\n",
    "    training_gdf = training_gdf.dropna(subset=['type'])\n",
    "    training_gdf['date'] = pd.to_datetime(training_gdf['date'], format='%Y%m')\n",
    "\n",
    "    # add year columnt\n",
    "    training_gdf['year'] = training_gdf['date'].dt.year\n",
    "\n",
    "    # convert column 'type' from string to int\n",
    "    training_gdf['type'] = training_gdf['type'].astype(int)\n",
    "    print('shape of training_gdf after drop NA:', training_gdf.shape)\n",
    "    \n",
    "    # training_gdf.to_parquet(gcs_path_df_long, filesystem=fs, compression='snappy') # i shouldn't export for now\n",
    "\n",
    "else:\n",
    "    training_gdf = gpd.read_parquet(gcs_path_df_long, filesystem=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5f85c-d2cd-4a75-8dde-2ab7cee1aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764b7e4-a4d8-40a9-8e07-f59266d473c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DISSOLVING, takes too long:\n",
    "# gdf_1_dissolved = training_gdf[training_gdf['type'] == 1].dissolve(by=['date', 'layer'])\n",
    "# gdf_0_dissolved = training_gdf[training_gdf['type'] == 0].dissolve(by=['date', 'layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b014e-411a-4f77-a518-bf6a095e1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().resolve().parents[1]  # adjust if your cwd differs\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from arcpy_arcgis_api_lib.xls_append.data_val import pandas_to_esri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf202c6-0d39-4459-a594-26e7f82ad414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n",
    "from arcpy_arcgis_api_lib.xls_append.data_val import pandas_to_esri\n",
    "\n",
    "def export_training_gdf_to_feature_class(\n",
    "    training_gdf: gpd.GeoDataFrame,\n",
    "    gdb_name: str = \"training_timeseries.gdb\",\n",
    "    feature_class_name: str = \"training_timeseries\",\n",
    "    workspace: str | None = None,\n",
    "    overwrite: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Persist `training_gdf` into a file geodatabase feature class.\n",
    "\n",
    "    - Creates the target GDB inside `workspace` (default: `arcpy.env.workspace`)\n",
    "    - Uses `arcgis.features.GeoAccessor` for conversion, so geometry/CRS are preserved\n",
    "    - Leverages `arcpy_arcgis_api_lib`â€™s `pandas_to_esri` mapping to preview ArcGIS field types\n",
    "    \"\"\"\n",
    "    if not isinstance(training_gdf, gpd.GeoDataFrame):\n",
    "        raise TypeError(\"training_gdf must be a GeoDataFrame\")\n",
    "\n",
    "    if training_gdf.crs is None:\n",
    "        raise ValueError(\"training_gdf is missing a CRS; set one before exporting\")\n",
    "\n",
    "    base_workspace = workspace or arcpy.env.workspace or arcpy.env.scratchWorkspace\n",
    "    if base_workspace is None:\n",
    "        raise ValueError(\"Set `arcpy.env.workspace` or pass a `workspace` path\")\n",
    "\n",
    "    gdb_path = Path(base_workspace) / gdb_name\n",
    "    if gdb_path.suffix.lower() != \".gdb\":\n",
    "        gdb_path = gdb_path.with_suffix(\".gdb\")\n",
    "    gdb_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not arcpy.Exists(str(gdb_path)):\n",
    "        arcpy.management.CreateFileGDB(str(gdb_path.parent), gdb_path.name)\n",
    "\n",
    "    esri_type_preview = {\n",
    "        col: pandas_to_esri.get(str(training_gdf[col].dtype), \"esriFieldTypeString\")\n",
    "        for col in training_gdf.columns\n",
    "        if col != training_gdf.geometry.name\n",
    "    }\n",
    "    display(pd.DataFrame.from_dict(esri_type_preview, orient=\"index\", columns=[\"esri_type\"]))\n",
    "\n",
    "    sedf = GeoAccessor.from_geodataframe(training_gdf)\n",
    "    output_fc = str(gdb_path / feature_class_name)\n",
    "\n",
    "    if arcpy.Exists(output_fc) and overwrite:\n",
    "        arcpy.management.Delete(output_fc)\n",
    "\n",
    "    sedf.spatial.to_featureclass(location=output_fc)\n",
    "    return output_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e473238-06f6-4307-a93c-540b9f8ac268",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_class_path = export_training_gdf_to_feature_class(\n",
    "    training_gdf,\n",
    "    gdb_name=\"training.gdb\",\n",
    "    feature_class_name=\"korindo_training_long\"\n",
    ")\n",
    "print(f\"Feature class saved to: {feature_class_path}\")\n",
    "print(arcpy.management.GetCount(feature_class_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b593d-05f9-4b3e-a4f9-f26c62535e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_class_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
